# Pre-training for Action Recognition with Automatically Generated Fractal Datasets

<div align="center">
  <video width="240" height="240" src="https://github.com/davidsvy/fractal_video/assets/57594395/c9a099bc-d676-46f1-bc72-9367c68490c1" controls ></video>
</div>



## Overview

Code for the paper "Pre-training for Action Recognition with Automatically Generated Fractal Datasets".


<p align="center">
  <image src="assets/fig-overview.png" />
</p>



Table of contents
===

<!--ts-->

- [Video Samples](#video-samples)
- [Installation](#installation)
- [Usage](#usage)
  - [Video Creation](#video-creation)
  - [Training](#training)
- [Contact](#contact)
  
 
<!--te-->


<a  id="video-samples"></a>
Video Samples
===

Examples of synthetic videos as well as the proposed augmentation schemes are displayed [here](video_examples.md).



<a  id="installation"></a>
Installation
===

```
pip install -r requirements.txt
```



<a  id="usage"></a>
Usage
===

<a  id="video-creation"></a>
## Video Creation

Unlabeled synthetic videos can be generated by running:
```
python generate_synthetic.py \
    --type [VIDEO_TYPE] \
    --samples [N_SAMPLES] \
    --dir_out [DIR_OUT]
```

In the above command:
* ```[VIDEO_TYPE]``` is the type of th produced video and should be delected from ```fractal, perlin, octopus, dead_leaves```.
* ```[N_SAMPLES]``` is the number of the produced samples. Alternatively, you can provide ```--time [TIME]``` to finish execution after a specified amount of time, eg ```--time 2h30m```.
* ```[DIR_OUT]``` is the directory where the produced videos will be stored.


To generate labeled videos, you should first sample parameters for classes:
```
python generate_synthetic.py \
    --type [VIDEO_TYPE] \
    --gen_param [N_CLASSES] \
    --dir_out [DIR_PARAM]

python generate_synthetic.py \
    --type [VIDEO_TYPE] \
    --samples [N_SAMPLES] \
    --dir_in [DIR_PARAM] \
    --dir_out [DIR_OUT]
```
In the above commands:
* ```[N_CLASSES]``` is the number of synthetic categories.
* ```[DIR_PARAM]``` is the directory where the parameters of the categories will be stored.

In case the code is not run on Linux with installed ffmpeg, add the argument ```--lib_save cv2```.



Additional aguments can be seen [here](src/synthetic/args.py).


<a  id="training"></a>
## Training

The first step download and prepare a dataset:

```
python prepare_data \
    --dataset [DATASET] \
    --root [ROOT]
```

In the above command:
* ```[DATASET]``` is the name of the dataset. Choices are ```hmdb51, ucf101, diving48, egtea, volleyball, scenes```. Additionally, you can choose one of the synthetic datasets which are listed [here](src/prepare_data/gdrive_ids.py). The prefix ```uns-``` indicates unlabeled datasets suitable for self-supervised learning whereas ```sup-``` denotes labaled datasets for classification.
* ```[ROOT]``` specifies the root directory where the dataset will be stored. Specifically ```[DATASET]``` will be stored at ```[ROOT]/[DATASET]```.


To train a model with a supervised objective, run:

```
python train_sup.py \
    --cfg [CFG] \
    --device [DEVICE] \
    --opts [OPTS]
```

In the above command:
* ```[CFG]``` is the path to the config file, e.g. ```cfg/hmdb51/tsm.yaml```.
* ```[DEVICE]``` is the GPU index. If nothing is provided, CPU will be used.
* ```[OPTS]``` is a list of arguments that overwrite ```[CFG]```. For instance:
  + ```TRAIN.OUTPUT [DIR]``` specifies that checkpoints & output logs will be stored at ```[DIR]```.
  + ```DATA.ROOT [ROOT]``` & ```DATA.DATASET [DATASET]``` indicate that the dataset files are stored at ```[ROOT]/[DATASET]```.
  + ```TRAIN.INIT_SSL [PATH_SSL]``` & ```TRAIN.INIT_SUP [PATH_SUP]``` indicate that the model weights will be initialized with a self-supervised checkpoint at ```[PATH_SSL]``` & supervised checkpoint at ```[PATH_SUP]``` respectively.
  + ```MODEL.RESUME [PATH]``` specifies to continue training & that model & optimizer weights should are initialized with a checkpoint found at ```[PATH]```.
  + ```MODEL.KINETICS [PATH]``` initializes the model with pre-trained Kinetics weights found at ```[PATH]```.


To train a model with a self-supervised objective, run:

```
python train_ssl.py \
    --cfg [CFG] \
    --device [DEVICE] \
    --opts [OPTS]
```

The following pre-trained checkpoints are provided:

| Dataset                  | Resolution | Weights  |
| ------------------------ | ---------- | ------------- |
| Fractal-Supervised       | 112        | [Link](https://drive.google.com/file/d/17VYbo9naUcSPhvbXu59vrPUlVIJ-j8Av/view?usp=sharing)     |
| Fractal-Supervised       | 224        | [Link](https://drive.google.com/file/d/1CBwq5T1cCQnOBctOjgOzPlBkOmvED0Yu/view?usp=sharing)     |
| Kinetics (off-the-shelf) | 224        | [Link](https://hanlab18.mit.edu/projects/tsm/models/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e100_dense.pth)     |




<a  id="contact"></a>
Contact
===
If you have any questions, please feel free to email the author at [dsvyez@gmail.com](mailto:dsvyez@gmail.com).
